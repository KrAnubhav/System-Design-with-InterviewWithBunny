# Lecture 9: Distributed Job Scheduler System Design

> **Lecture:** 09  
> **Topic:** System Design  
> **Application:** Distributed Job Scheduler (Cron, Airflow, Jenkins)  
> **Scale:** 10,000 Jobs/Second  
> **Difficulty:** Hard  
> **Previous:** [[Lecture-08-Chat-Application-System-Design|Lecture 8: Chat Application]]

---

## ğŸ“‹ Table of Contents

1. [Brief Introduction](#1-brief-introduction)
2. [Functional and Non-Functional Requirements](#2-functional-and-non-functional-requirements)
3. [Core Entities](#3-core-entities)
4. [API Design](#4-api-design)
5. [High-Level Design (HLD)](#5-high-level-design-hld)
6. [Low-Level Design (LLD)](#6-low-level-design-lld)
7. [Complete Architecture Diagram](#7-complete-architecture-diagram)
8. [Database Indexing & Partitioning](#8-database-indexing--partitioning)
9. [Interview Q&A](#9-interview-qa)
10. [Key Takeaways](#10-key-takeaways)

---

<br>

## 1. Brief Introduction

### What is a Distributed Job Scheduler?

A **distributed job scheduler** is a system that allows users to create, schedule, and execute repetitive or one-time jobs at specific times across multiple servers.

**Examples:** Cron, Apache Airflow, Jenkins, Kubernetes CronJobs, AWS Batch

### Problem It Solves

1. **Scheduled Execution:** Run jobs at specific times
2. **Recurring Jobs:** Daily, weekly, monthly tasks
3. **Immediate Execution:** Run jobs on-demand
4. **Monitoring:** Track job status in real-time
5. **Retry Logic:** Handle failures gracefully
6. **Scalability:** Handle thousands of concurrent jobs

**Use Cases:**
- **Data Pipelines:** ETL jobs every night at 2 AM
- **Database Cleanup:** Delete old records every Sunday
- **Report Generation:** Monthly sales reports
- **Backup Jobs:** Daily database backups
- **Email Campaigns:** Send newsletters every Friday

---

## 2. Functional and Non-Functional Requirements

### 2.1 Functional Requirements

1. **Create and Schedule Jobs**
   - Run immediately
   - Run at future time
   - Run on schedule (cron expression)

2. **Monitor Jobs**
   - Real-time status updates
   - View job history
   - See error logs

3. **Cancel and Update Jobs**
   - Cancel scheduled jobs
   - Cancel running jobs
   - Update job schedule

âš ï¸ **Out of Scope:** Job dependencies (DAGs like Airflow)

### 2.2 Non-Functional Requirements

1. **Scale**
   - **10,000 jobs/second** (peak traffic)
   - Support concurrent execution

2. **CAP Theorem**
   - **Highly Available (AP):** Zero downtime
   - **Eventual Consistency:** Slight delay in job listing is acceptable
   - **Trade-off:** Availability > Consistency

   **Why?**
   - âœ… Job scheduler must always be available
   - âœ… Eventual consistency is acceptable (job appears in dashboard after 1-2 seconds)

3. **At-Least-Once Execution**
   - **Every scheduled job must run at least once**
   - No job should be skipped

4. **Low Latency**
   - **Execute jobs within 2 seconds of scheduled time**
   - Example: Job scheduled at 5:30:00 â†’ Must start by 5:30:02

---

## 3. Core Entities

1. **Job** - Task to be executed
2. **Scheduler** - Determines when to run jobs
3. **Executor** - Runs the actual job

---

## 4. API Design

### 4.1 Create Job

```
POST /v1/jobs
```

**Request Body:**
```json
{
  "name": "Daily Cleanup",
  "scheduleType": "cron",  // immediate, future, cron
  "scheduleTime": "2026-01-22T02:00:00Z",  // For future jobs
  "cronExpression": "0 0 2 * * *",  // For cron jobs
  "payload": {
    "script": "cleanup.py",
    "params": {"days": 30}
  },
  "maxRetries": 3
}
```

**Response:**
```json
{
  "jobId": "job_123",
  "status": "scheduled"
}
```

---

### 4.2 Get Job Details

```
GET /v1/jobs/{jobId}
```

**Response:**
```json
{
  "jobId": "job_123",
  "name": "Daily Cleanup",
  "scheduleType": "cron",
  "cronExpression": "0 0 2 * * *",
  "status": "scheduled",
  "createdAt": "2026-01-21T12:00:00Z"
}
```

---

### 4.3 Get Job Status

```
GET /v1/jobs/{jobId}/status
```

**Response:**
```json
{
  "jobId": "job_123",
  "status": "running",  // queued, running, success, failed
  "startTime": "2026-01-22T02:00:01Z",
  "progress": "50%",
  "executorId": "executor_456"
}
```

---

### 4.4 Update Job

```
PUT /v1/jobs/{jobId}
```

**Request Body:**
```json
{
  "cronExpression": "0 0 3 * * *",  // Change to 3 AM
  "maxRetries": 5
}
```

---

### 4.5 Cancel Job

```
DELETE /v1/jobs/{jobId}
```

**Response:**
```json
{
  "jobId": "job_123",
  "status": "cancelled"
}
```

âš ï¸ **Note:** Can cancel both scheduled and running jobs

---

### 4.6 Run Job Immediately

```
POST /v1/jobs/{jobId}/run
```

**Response:**
```json
{
  "jobId": "job_123",
  "runId": "run_789",
  "status": "queued"
}
```

---

## 5. High-Level Design (HLD)

### 5.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Users                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  API Gateway    â”‚
                â”‚  Load Balancer  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
        â–¼                â–¼                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Job    â”‚    â”‚  Search  â”‚    â”‚ Watcher  â”‚
  â”‚ Service  â”‚    â”‚ Service  â”‚    â”‚ Service  â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚                â”‚
       â–¼               â–¼                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Kafka   â”‚    â”‚  Job DB  â”‚    â”‚  Kafka   â”‚
  â”‚          â”‚    â”‚(Postgres)â”‚    â”‚  (Queue) â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                                â”‚
       â–¼                                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Consumer â”‚                    â”‚   Job    â”‚
  â”‚ Service  â”‚                    â”‚ Consumer â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                                â”‚
       â–¼                                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Job DB  â”‚                    â”‚ Executor â”‚
  â”‚(Postgres)â”‚                    â”‚ Service  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Service Responsibilities

**Job Service:**
- Create, update, delete jobs
- Trigger immediate execution

**Search Service:**
- Get job details
- Get job status

**Watcher Service:**
- Scan Job DB every 20 seconds
- Fetch jobs to run in next 5 minutes
- Push to Kafka queue

**Job Consumer:**
- Consume jobs from Kafka
- Send to Executor Service

**Executor Service:**
- Execute actual job
- Update status every 10 seconds

---

## 6. Low-Level Design (LLD)

### 6.1 Job Service (Create/Update/Delete)

**Flow:**

```
User creates job
    â†“
POST /v1/jobs
    â†“
Job Service
    â†“
Publish to Kafka (job_events topic)
    â†“
Consumer Service
    â†“
Insert into Job DB
```

**Why Kafka?**
- âœ… Handle 10,000 jobs/second
- âœ… Decouple write operations
- âœ… Prevent database overload

---

### 6.2 Job Database (PostgreSQL)

**Why PostgreSQL?**
- âœ… Relational data (jobs, runs, schedules)
- âœ… ACID compliance
- âœ… Indexing support

#### **Table 1: Jobs**

```sql
CREATE TABLE jobs (
    job_id UUID PRIMARY KEY,
    name VARCHAR(255),
    schedule_type VARCHAR(20),  -- immediate, future, cron
    schedule_time TIMESTAMP,  -- For future jobs
    cron_expression VARCHAR(100),  -- For cron jobs
    status VARCHAR(20),  -- scheduled, paused, cancelled
    payload JSONB,  -- Job parameters
    max_retries INT DEFAULT 3,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);

-- Index for Watcher Service
CREATE INDEX idx_schedule_time ON jobs(schedule_time);
CREATE INDEX idx_job_id ON jobs(job_id);
```

**Cron Expression Format:**

```
* * * * * *
â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€ Day of week (0-6, Sunday=0)
â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€ Month (1-12)
â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€ Day of month (1-31)
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€ Hour (0-23)
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Minute (0-59)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Second (0-59)
```

**Examples:**
```
0 0 2 * * *   â†’ Every day at 2:00 AM
0 30 9 * * 1  â†’ Every Monday at 9:30 AM
0 0 0 1 * *   â†’ First day of every month at midnight
```

---

#### **Table 2: Job Runs**

```sql
CREATE TABLE job_runs (
    id SERIAL PRIMARY KEY,  -- Auto-increment
    job_id UUID REFERENCES jobs(job_id),
    status VARCHAR(20),  -- queued, running, success, failed
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    modified_time TIMESTAMP,  -- Last heartbeat
    executor_id VARCHAR(100),
    attempt_number INT,
    error_message TEXT,
    created_at TIMESTAMP
);

-- Composite index for Watcher Service
CREATE INDEX idx_status_modified ON job_runs(status, modified_time);
CREATE INDEX idx_job_id ON job_runs(job_id);
```

**Why `modified_time`?**
- Track last heartbeat from Executor
- Detect dead executors (no update for 15+ seconds)

---

### 6.3 Watcher Service (The Heart of the System!)

**Responsibilities:**

1. **Fetch upcoming jobs** (every 20 seconds)
2. **Detect dead executors** (check `modified_time`)

---

#### **Task 1: Fetch Upcoming Jobs**

**Algorithm:**

```python
def fetch_upcoming_jobs():
    # Run every 20 seconds
    while True:
        current_time = now()
        future_time = current_time + 5 minutes
        
        # Fetch jobs to run in next 5 minutes
        jobs = db.query("""
            SELECT * FROM jobs
            WHERE schedule_time BETWEEN ? AND ?
            AND status = 'scheduled'
        """, current_time, future_time)
        
        # Push to Kafka
        for job in jobs:
            kafka.publish("run_queue", job)
            
            # Insert into job_runs table
            db.execute("""
                INSERT INTO job_runs (job_id, status, created_at)
                VALUES (?, 'queued', NOW())
            """, job.job_id)
        
        sleep(20 seconds)
```

**Why 5-minute window?**
- Balance between database load and latency
- Ensures jobs are picked up in time

**Why 20-second interval?**
- Prevents database overload
- Acceptable latency for most use cases

---

#### **Task 2: Detect Dead Executors**

**Problem:** Executor crashes while running job

**Solution:** Check `modified_time`

**Algorithm:**

```python
def detect_dead_executors():
    # Run every 20 seconds
    while True:
        threshold = now() - 15 seconds
        
        # Find jobs with stale heartbeat
        stale_jobs = db.query("""
            SELECT * FROM job_runs
            WHERE status = 'running'
            AND modified_time < ?
        """, threshold)
        
        # Re-queue for retry
        for job in stale_jobs:
            kafka.publish("retry_queue", job)
            
            # Increment attempt number
            db.execute("""
                UPDATE job_runs
                SET attempt_number = attempt_number + 1
                WHERE id = ?
            """, job.id)
        
        sleep(20 seconds)
```

**Why 15 seconds?**
- Executor sends heartbeat every 10 seconds
- 5-second buffer for network delays

---

### 6.4 Kafka Queues

**Three Queues:**

```
1. run_queue â†’ Jobs ready to execute
2. retry_queue â†’ Failed jobs to retry
3. dead_letter_queue â†’ Jobs that exceeded max retries
```

**Why Kafka?**
- âœ… High throughput (10,000 jobs/second)
- âœ… Persistent (no message loss)
- âœ… Ordered (FIFO)

---

### 6.5 Job Consumer Service

**Responsibilities:**

1. Consume jobs from Kafka
2. Send to Executor Service
3. Update job status

**Flow:**

```python
def consume_jobs():
    while True:
        # Consume from run_queue
        job = kafka.consume("run_queue")
        
        # Send to Executor Service
        executor = get_least_loaded_executor()
        executor.execute(job)
        
        # Update status to "running"
        db.execute("""
            UPDATE job_runs
            SET status = 'running',
                start_time = NOW(),
                executor_id = ?
            WHERE job_id = ?
        """, executor.id, job.job_id)
```

---

### 6.6 Executor Service

**Responsibilities:**

1. Execute job
2. Send heartbeat every 10 seconds
3. Update final status (success/failed)

**Flow:**

```python
def execute_job(job):
    # Start execution
    publish_status(job.id, "running", start_time=now())
    
    # Heartbeat thread
    heartbeat_thread = start_heartbeat(job.id)
    
    try:
        # Execute actual job
        result = run_script(job.payload.script, job.payload.params)
        
        # Success
        publish_status(job.id, "success", end_time=now())
        
    except Exception as e:
        # Failure
        publish_status(job.id, "failed", error=str(e))
        
        # Retry logic
        if job.attempt_number < job.max_retries:
            kafka.publish("retry_queue", job)
        else:
            kafka.publish("dead_letter_queue", job)
    
    finally:
        stop_heartbeat(heartbeat_thread)


def start_heartbeat(job_id):
    def heartbeat():
        while True:
            publish_status(job_id, "running", modified_time=now())
            sleep(10 seconds)
    
    thread = Thread(target=heartbeat)
    thread.start()
    return thread


def publish_status(job_id, status, **kwargs):
    kafka.publish("status_updates", {
        "job_id": job_id,
        "status": status,
        **kwargs
    })
```

**Why heartbeat?**
- Detect dead executors
- Show real-time progress

---

### 6.7 Status Update Flow

**Flow:**

```
Executor Service
    â†“
Publish to Kafka (status_updates topic)
    â†“
Consumer Service
    â†“
Update job_runs table
```

**Consumer Service:**

```python
def consume_status_updates():
    while True:
        update = kafka.consume("status_updates")
        
        db.execute("""
            UPDATE job_runs
            SET status = ?,
                start_time = ?,
                end_time = ?,
                modified_time = ?,
                error_message = ?
            WHERE job_id = ?
        """, update.status, update.start_time, update.end_time,
             update.modified_time, update.error_message, update.job_id)
```

---

### 6.8 Retry Logic

**Three Scenarios:**

**1. Job Fails (Executor Detects):**
```
Executor catches exception
    â†“
Publish to retry_queue
    â†“
Job Consumer re-executes
```

**2. Executor Crashes:**
```
Watcher Service detects stale modified_time
    â†“
Publish to retry_queue
    â†“
Job Consumer re-executes
```

**3. Max Retries Exceeded:**
```
attempt_number >= max_retries
    â†“
Publish to dead_letter_queue
    â†“
Mark job as "failed"
```

---

### 6.9 Cancel Running Job

**Challenge:** How to stop a running job?

**Solution:** Redis Cache

**Flow:**

```
1. User clicks "Cancel"
    â†“
2. Job Service publishes to Kafka (cancel_requests)
    â†“
3. Consumer Service inserts into Redis:
    SET cancel:job_123 "true" EX 30
    â†“
4. Executor Service checks Redis every 10 seconds:
    IF redis.get("cancel:job_123"):
        stop_execution()
        publish_status("cancelled")
```

**Why Redis?**
- âœ… Fast lookup (< 1ms)
- âœ… TTL support (auto-expire after 30 seconds)

**Why TTL?**
- Prevent stale cancel requests
- Only running jobs should be affected

---

### 6.10 Run Job Immediately

**Flow:**

```
User clicks "Run Now"
    â†“
Job Service checks schedule_time
    â†“
IF schedule_time <= now() + 1 minute:
    Publish directly to run_queue (bypass Watcher)
ELSE:
    Insert into Job DB (Watcher will pick up)
```

**Why bypass Watcher?**
- Watcher scans every 20 seconds
- Immediate jobs can't wait

---

### 6.11 Edge Case: Race Condition

**Problem:**

```
Watcher scans at 5:00:00 â†’ Fetches jobs for 5:00:00 - 5:05:00
User creates job at 5:00:10 (scheduled for 5:00:15)
Watcher scans at 5:00:20 â†’ Fetches jobs for 5:00:20 - 5:05:20

Job scheduled at 5:00:15 is MISSED! âŒ
```

**Solution:**

```python
# Job Service checks schedule_time
if schedule_time <= now() + 1 minute:
    # Bypass Watcher, publish directly to run_queue
    kafka.publish("run_queue", job)
else:
    # Let Watcher handle it
    kafka.publish("job_events", job)
```

---

### 6.12 Watcher Service State Management

**Problem:** Watcher crashes, loses track of last scan time

**Solution:** Redis Cache

**Flow:**

```python
def fetch_upcoming_jobs():
    # Get last scan time from Redis
    last_scan = redis.get("watcher:last_scan") or now()
    
    current_time = now()
    future_time = current_time + 5 minutes
    
    # Fetch jobs
    jobs = db.query("""
        SELECT * FROM jobs
        WHERE schedule_time BETWEEN ? AND ?
    """, last_scan, future_time)
    
    # Update last scan time
    redis.set("watcher:last_scan", current_time)
```

**Why Redis?**
- âœ… Persistent state
- âœ… Fast read/write

---

## 7. Complete Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USERS                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  API Gateway    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
        â–¼                â–¼                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Job    â”‚    â”‚  Search  â”‚    â”‚ Watcher  â”‚
  â”‚ Service  â”‚    â”‚ Service  â”‚    â”‚ Service  â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚                â”‚
       â”‚               â”‚                â”‚
       â–¼               â–¼                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚           Kafka Broker                    â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
  â”‚  â”‚job_eventsâ”‚run_queue â”‚retry_queueâ”‚     â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
       â”‚                                  â”‚
       â–¼                                  â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Consumer â”‚                      â”‚   Job    â”‚
  â”‚ Service  â”‚                      â”‚ Consumer â”‚
  â”‚ (Write)  â”‚                      â”‚          â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                                  â”‚
       â–¼                                  â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚           PostgreSQL (Job DB)             â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
  â”‚  â”‚   jobs   â”‚ job_runs â”‚                 â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Executor â”‚
                  â”‚ Service  â”‚
                  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Redis   â”‚
                  â”‚  Cache   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. Database Indexing & Partitioning

### 8.1 Jobs Table

**Indexes:**
```sql
CREATE INDEX idx_job_id ON jobs(job_id);  -- Primary lookup
CREATE INDEX idx_schedule_time ON jobs(schedule_time);  -- Watcher scans
```

**Partitioning:**
```sql
-- Partition by schedule_time (monthly)
CREATE TABLE jobs_2026_01 PARTITION OF jobs
FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');

CREATE TABLE jobs_2026_02 PARTITION OF jobs
FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');
```

**Why partition by schedule_time?**
- Watcher Service queries by schedule_time
- Faster scans (only relevant partition)

---

### 8.2 Job Runs Table

**Indexes:**
```sql
CREATE INDEX idx_job_id ON job_runs(job_id);  -- Lookup by job
CREATE INDEX idx_status_modified ON job_runs(status, modified_time);  -- Detect dead executors
```

**Composite Index Explained:**

```sql
-- Watcher Service query
SELECT * FROM job_runs
WHERE status = 'running'
AND modified_time < '2026-01-21T12:00:00Z';

-- Uses idx_status_modified (composite index)
```

---

## 9. Interview Q&A

### Q1: Why Kafka instead of direct database writes?

**A:**

**Without Kafka:**
```
10,000 jobs/second â†’ Direct DB writes
    â†“
Database overload âŒ
```

**With Kafka:**
```
10,000 jobs/second â†’ Kafka (buffering)
    â†“
Consumer Service â†’ Batch writes to DB
    â†“
Database handles load âœ…
```

**Benefits:**
- âœ… Decouple write operations
- âœ… Handle traffic spikes
- âœ… Prevent database overload

### Q2: Why scan every 20 seconds (not 1 second)?

**A:**

**Trade-off:**

| Interval | Database Load | Latency |
|----------|---------------|---------|
| 1 second | Very High âŒ | Very Low âœ… |
| 20 seconds | Low âœ… | Acceptable âœ… |
| 60 seconds | Very Low âœ… | High âŒ |

**Decision:** 20 seconds (balance load and latency)

**Mitigation for immediate jobs:**
- Bypass Watcher for jobs scheduled within 1 minute

### Q3: How to handle Watcher Service failure?

**A:**

**Problem:** Watcher crashes, loses state

**Solution:** Redis state management

```python
# Before crash
redis.set("watcher:last_scan", "2026-01-21T12:00:00Z")

# After restart
last_scan = redis.get("watcher:last_scan")
# Resume from last scan time
```

### Q4: How to prevent duplicate job execution?

**A:**

**Problem:** Watcher scans twice, picks same job

**Solution:** Idempotency check

```python
def fetch_upcoming_jobs():
    jobs = db.query("SELECT * FROM jobs WHERE ...")
    
    for job in jobs:
        # Check if already queued
        existing = db.query("""
            SELECT * FROM job_runs
            WHERE job_id = ? AND status = 'queued'
        """, job.job_id)
        
        if not existing:
            kafka.publish("run_queue", job)
```

### Q5: Why heartbeat every 10 seconds?

**A:**

**Purpose:**
- Detect dead executors
- Show real-time progress

**Why 10 seconds?**
- Balance between database load and detection speed
- 15-second threshold = 10s heartbeat + 5s buffer

### Q6: How to handle max retries exceeded?

**A:**

**Flow:**

```
Job fails â†’ attempt_number++
    â†“
IF attempt_number < max_retries:
    Publish to retry_queue
ELSE:
    Publish to dead_letter_queue
    Mark job as "failed"
    Send alert to user
```

**Dead Letter Queue:**
- Manual intervention required
- Admin reviews and re-triggers

### Q7: How to cancel a scheduled (not running) job?

**A:**

**Flow:**

```
User clicks "Cancel"
    â†“
Job Service updates Job DB:
    UPDATE jobs SET status = 'cancelled'
    WHERE job_id = ?
    â†“
Watcher Service skips cancelled jobs:
    SELECT * FROM jobs
    WHERE status = 'scheduled'  -- Excludes cancelled
```

### Q8: Kafka vs Redis Sorted Set?

**A:**

**Problem with Kafka:**
```
Jobs in queue: [5:00, 5:02, 5:04, 5:06]
New job arrives: 5:01
    â†“
Inserted at end: [5:00, 5:02, 5:04, 5:06, 5:01]
    â†“
Job at 5:01 executes LAST! âŒ
```

**Solution: Redis Sorted Set**

```python
# Insert with score = schedule_time
redis.zadd("job_queue", {job_id: schedule_time})

# Consume in order
jobs = redis.zrangebyscore("job_queue", 0, now())
```

**Comparison:**

| Feature | Kafka | Redis Sorted Set |
|---------|-------|------------------|
| Ordering | FIFO | Sorted by score âœ… |
| Throughput | Very High | High |
| Persistence | Disk | Memory |

**Decision:** Redis Sorted Set (better ordering)

**Alternative:** Amazon SQS (supports delayed messages)

### Q9: How to handle job logs?

**A:**

**Flow:**

```
Executor Service
    â†“
Write logs to file: /logs/job_123.log
    â†“
Upload to S3: s3://logs/job_123.log
    â†“
Update job_runs table:
    error_message = "s3://logs/job_123.log"
```

**User retrieves logs:**
```
GET /v1/jobs/{jobId}/logs
    â†“
Return S3 URL
```

### Q10: How to scale Executor Service?

**A:**

**Strategies:**

1. **Horizontal Scaling:**
   - Add more Executor instances
   - Job Consumer distributes load

2. **Resource Isolation:**
   - Docker containers per job
   - Prevent resource contention

3. **Priority Queues:**
   - High-priority jobs â†’ Separate queue
   - Dedicated executors

---

## 10. Key Takeaways

âœ… **Watcher Service:** Scans DB every 20 seconds, fetches jobs for next 5 minutes  
âœ… **Kafka Queues:** run_queue, retry_queue, dead_letter_queue  
âœ… **Heartbeat:** Executor sends status every 10 seconds  
âœ… **Dead Executor Detection:** Check `modified_time` > 15 seconds  
âœ… **Retry Logic:** Max retries, then dead letter queue  
âœ… **Cancel Running Job:** Redis cache with TTL  
âœ… **Immediate Jobs:** Bypass Watcher, publish directly to Kafka  
âœ… **Race Condition:** Jobs within 1 minute â†’ Direct to queue  
âœ… **State Management:** Redis stores last scan time  
âœ… **Ordering:** Redis Sorted Set > Kafka (for time-based ordering)  

---

## Summary

**Architecture Highlights:**
- 6+ microservices
- 2 databases (PostgreSQL for jobs, Redis for state)
- Kafka for event streaming
- Watcher Service (scans every 20 seconds)
- Executor Service (runs jobs, sends heartbeat)

**Job Lifecycle:**
```
Create â†’ Kafka â†’ Job DB â†’ Watcher â†’ Kafka Queue â†’ Consumer â†’ Executor â†’ Status Updates
```

**Retry Flow:**
```
Job Fails â†’ retry_queue â†’ Consumer â†’ Executor (attempt++)
Max Retries â†’ dead_letter_queue â†’ Manual intervention
```

**Dead Executor Detection:**
```
Executor crashes â†’ No heartbeat â†’ modified_time stale â†’ Watcher detects â†’ Re-queue
```

**Performance:**
- 10,000 jobs/second
- < 2 seconds latency
- At-least-once execution

**End of Lecture 9**
